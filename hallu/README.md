
Hallucinations related tools

```
  Shared Tools and documents related to
  Hallucination detection, correction related tools
  for internal uses.




```

#### Tools
  /tools






#### Recent Publications
```

From Single to Multi:How LLMs Hallucinate in Multi-Document Summarization (Oct 2024)
https://arxiv.org/pdf/2410.13961v1

Repo: https://github.com/megagonlabs/Hallucination_MDS


Steering Knowledge Selection Behaviours in LLMs via SAE-Based Representation Engineering
https://arxiv.org/pdf/2410.15999
https://github.com/yuzhaouoe/SAE-based-representation-engineering



Tofu Eval: Evaluating Hallucinations of LLMs on Topic-Focused Dialogue Summarization
https://arxiv.org/pdf/2402.13249





Hallucination of Multimodal Large Language Models: A Survey, 2024
https://arxiv.org/pdf/2404.18930



ON MEMORIZATION OF LARGE LANGUAGE MODELS IN LOGICAL REASONING
https://arxiv.org/pdf/2410.23123


Ji et al. “Survey of hallucination in natural language generation.” ACM Computing Surveys (2022)

[2] Gekhman et al. “Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?” arXiv preprint arXiv:2405.05904 (2024).

[3] Min et al. “FActScore: Fine-grained atomic evaluation of factual precision in long form text generation.” EMNLP 2023.

[4] Wei et al. 2024 “Long-form Factuality in LLMs” arXiv preprint arXiv:2403.18802 (2024).

[5] Chern et al. “FacTool: Factuality detection in generative AI - a tool augmented framework for multi-task and multi-domain scenarios.” arXiv preprint arXiv:2307.13528 (2023).

[6] Lin et al. “TruthfulQA: Measuring How Models Mimic Human Falsehoods.” ACL 2022.

[7] Yin et al. “Do Large Language Models Know What They Don’t Know?” ACL 2023.

[8] Kadavath et al. “Language Models (Mostly) Know What They Know” arXiv preprint arXiv:2207.05221 (2022).

[9] Agrawal et al. “Do language models know when they’re hallucinating references?” arXiv preprint arXiv:2305.18248 (2023).

[10] Lin et al. “Teaching Models to Learn Uncertainty in Words.” arXiv preprint arXiv:2205.14334 (2022).

[11] Gao et al. “RARR: Researching and Revising What Language Models Say, Using Language Models.” ACL 2023.

[12] He et al. “Rethinking with retrieval: Faithful large language model inference.” arXiv preprint arXiv:2301.00303 (2022).

[13] Asai et al. “Self-RAG: Learning to retrieve, generate and critique through self-reflection.” ICLR 2024.

[14] Mishra et al. “Fine-grained Hallucination Detection and Editing for Language Models.” arXiv preprint arXiv:2401.06855 (2024).

[15] Lee, et al. “Factuality Enhanced Language Models for Open-Ended Text Generation.” NeuriPS 2022.

[16] Manakul et al. “SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models.” EMNLP 2023.

[17] Li et al. “Inference-Time Intervention: Eliciting Truthful Answers from a Language Model.” NeuriPS 2023.

[18] Chuang et al. “DoLa: Decoding by contrasting layers improves factuality in large language models.” ICLR 2024.

[19] Dhuliawala et al. “Chain-of-Verification Reduces Hallucination in Large Language Models.” arXiv preprint arXiv:2309.11495 (2023).

[20] Sun et al. “Recitation-Augmented Language Models.” ICLR 2023.

[21] Lin et al. “FLAME: Factuality-Aware Alignment for Large Language Models.” arXiv preprint arXiv:2405.01525 (2024).

[22] Tian & Mitchell et al. “Fine-tuning Language Models for Factuality.” ICLR 2024. 

[24] Menick et al. “Teaching language models to support answers with verified quotes.” arXiv preprint arXiv:2203.11147 

```


